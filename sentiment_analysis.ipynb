{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0231a5fa",
   "metadata": {},
   "source": [
    "# Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81360f14",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15320/5168946.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m  \u001b[1;31m# for searching common words in a string\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15320/5168946.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m  \u001b[1;31m# for searching common words in a string\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import re  # for searching common words in a string\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69083c6",
   "metadata": {},
   "source": [
    "# Installing vader sentiment analyser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c826bdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c127dd",
   "metadata": {},
   "source": [
    "# Importing Vader sentiment analyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568c28da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791dc966",
   "metadata": {},
   "source": [
    "# Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f21dacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv ('IPL_2022_tweets.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d41d3e",
   "metadata": {},
   "source": [
    "# Analysing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a24e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285ec5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9173e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164f4148",
   "metadata": {},
   "source": [
    "# Copying text to perform EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14ea377",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['senttext'] = df['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c385432b",
   "metadata": {},
   "source": [
    "# Conterting all string data to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4d426e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.apply(lambda x: x.astype(str).str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15297476",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_df = df[df.user_location != 'nan']\n",
    "loc_df.user_location.value_counts().nlargest(20).plot(kind='bar',figsize=(25,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebb26c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "indian_cities = {}\n",
    "indian_ipl_cities = ['mumbai','bangalore','chennai','delhi','kolkata','lucknow','ahmedabad','hyderabad','punjab','jaipur']\n",
    "for city in indian_ipl_cities:\n",
    "    indian_cities[city] = df.user_location.str.count(city).sum()\n",
    "    \n",
    "plt.figure(figsize=(25,10))\n",
    "plt.bar(*zip(*indian_cities.items()))\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7072da",
   "metadata": {},
   "source": [
    "Checking if the user account is verified or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95f7f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.user_verified.value_counts().nlargest(2).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0cb0e1",
   "metadata": {},
   "source": [
    "This shows that most of the users are not verified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b839be",
   "metadata": {},
   "source": [
    "# Checking most commonly using hastags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68e12f99",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15320/2887147612.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhashtag_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhashtags\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'nan'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mhashtag_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhashtags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlargest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'bar'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15320/2887147612.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhashtag_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhashtags\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'nan'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mhashtag_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhashtags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlargest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'bar'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "hashtag_df = df[df.hashtags != 'nan']\n",
    "hashtag_df.hashtags.value_counts().nlargest(5).plot(kind='bar', rot=0, figsize=(20,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6ada23",
   "metadata": {},
   "source": [
    "#ipl2022 is most commonly used hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ff5d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfb1e8a",
   "metadata": {},
   "source": [
    "#  Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3948d55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "df.text = df.text.apply(lambda x:' '.join([word for word in x.split() if word not in (stop_words)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b21ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text = df.text.apply(lambda x: ' '.join(re.sub(\"(@[A-Za-z0-9]+)|(#[A-Za-z0-9]+)\", \" \", x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d847266b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text = df.text.apply(lambda x: ' '.join(re.sub(\"[\\.\\,\\!\\?\\:\\;\\-\\=\\_\\'\\*\\\"|(|)]\", \" \", x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d228988b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text = df.text.apply(lambda x: ' '.join(re.sub(r'http\\S+', '',x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589bdc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd8fb46",
   "metadata": {},
   "source": [
    "Data is cleaned except emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30817594",
   "metadata": {},
   "source": [
    "# Creating a wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad361ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(\n",
    "                          background_color='white',\n",
    "                          colormap='Reds',\n",
    "                          max_words=200,\n",
    "                          max_font_size=40, \n",
    "                          random_state=49\n",
    "                         ).generate(str(df['text']))\n",
    "\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e957a0f",
   "metadata": {},
   "source": [
    "# Replacing the emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af085d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # UCS-4\n",
    "    e = re.compile(u'[\\U00010000-\\U0010ffff]')\n",
    "except re.error:\n",
    "    # UCS-2\n",
    "    e = re.compile(u'[\\uD800-\\uDBFF][\\uDC00-\\uDFFF]')\n",
    "emojis = []\n",
    "for x in df.text:\n",
    "    match  = e.search(x)\n",
    "    if match:\n",
    "        emojis.append(match.group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c5bef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfe =  pd.DataFrame(emojis,columns=['text'])\n",
    "pd.Series(' '.join(dfe['text']).lower().split()).value_counts()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ed9db7",
   "metadata": {},
   "source": [
    "# Finding similar words using word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42698909",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 400    # Word vector dimensionality                      \n",
    "min_word_count = 5   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "wt = [list(x.split()) for x in df.text]\n",
    "from gensim.models import word2vec\n",
    "print (\"Training model...\")\n",
    "wv_model = word2vec.Word2Vec(wt, workers=num_workers, \\\n",
    "            vector_size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "wv_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b60585",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_model.wv.most_similar(\"ipl\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b4b542",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_model.wv.most_similar(\"dhoni\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f08751",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_model.wv.most_similar(\"captain\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2412ddc",
   "metadata": {},
   "source": [
    "# Now applying VADER sentiment analyser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88a6615",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyser = SentimentIntensityAnalyzer()\n",
    "df['sentiment_score'] = df['senttext'].apply(lambda x: analyser.polarity_scores(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f2053e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_func(sentiment):\n",
    "    for k,v in sentiment.items():\n",
    "        if (k== 'pos' or k or 'neg' or k == 'neu') == True:\n",
    "            if (sentiment['pos'] > 0.5 and sentiment['neg'] < 0.5 and sentiment['neu'] < 0.5) == True:\n",
    "                return 'positive'\n",
    "            elif (sentiment['pos'] < 0.5 and sentiment['neg'] > 0.5 and sentiment['neu'] < 0.5) == True:\n",
    "                return 'negative'\n",
    "            elif (sentiment['pos'] < 0.5 and sentiment['neg'] < 0.5 and sentiment['neu'] > 0.5) == True:\n",
    "                return 'neutral'\n",
    "\n",
    "df['sentiment'] = df['sentiment_score'].apply(sentiment_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232444c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sentiment.value_counts().plot(kind='bar', rot=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c29f070",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3820e9",
   "metadata": {},
   "source": [
    "Most the tweets are neutral. This can be due to most tweets just containing score updates or match updates.\n",
    "\n",
    "Number of positive tweets are more than negative. Seems like people were very happy with tournament happening at such difficult time and people got excited and happy to see their favorite cricketers back on pitch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917c9d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92c4c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = input(\"Enter your value: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d384dd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "analyser2 = SentimentIntensityAnalyzer()\n",
    "sentiment = analyser2.polarity_scores(val)\n",
    "sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132da1e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
